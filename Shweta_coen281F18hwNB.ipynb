{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3  (Due: 10/31/2018)\n",
    "\n",
    "COEN 281, Fall 2018  \n",
    "Professor Marwah\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this HW is to implement a Naive Bayes classifier to predict whether a tweet was posted by a Republican or Democrat politician. The training data consist of about 13K tweets collected before the 2006 US presidential elections, There are about an equal number of Republican and Democrat tweets, and the tweets belong to three republican and three democrat twitter accounts. \n",
    "\n",
    "To represent each tweet, we will use a commonly used model in natural language processing called 'bag of words' model. A bag of words representation of a document (tweet here) consists of words and their frequencies in the document. The order of words is ignored.  \n",
    "\n",
    "There four main tasks.\n",
    "1. Tokenization: Parsing and converting the tweets to tokens. [**This is already done for you**]\n",
    "2. Feature matrix construction from the training data set\n",
    "3. Learning Naive Bayes parameters, priors and likelihoods, from the feature matrix.\n",
    "4. Using the learned NB model to predict the labels of the test data set (about 4K tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "This task consists of converting each tweet into a sequence of \"tokens\" that can be used as features. Tokens are essentially characters and character sequences obtained after using white space as a separator. A lot these are noise that we want to remove; some are words or other character sequences that are useful features. A python package called *NLTK* (natural language toolkit) contains several tokenizers, including one for tweets. We use that tokenizer; in addition we do the following:\n",
    "- remove stopwords. These are words that are frequently used in a language but do not carry any semantic information, e.g., the, an , a, this, is, was, etc.\n",
    "- make all tokens lower case (this is done by the tweet tokenizer)\n",
    "- removing twitter handles (again, done by the tweet tokenizer)\n",
    "- remove punctuations, http links\n",
    "\n",
    "Finally, we \"lemmatize\" the tokens. That means we convert different forms of a word to a common basic form, so that they can be recognized as the same work. E.g., vote, votes, voted would all be converted to vote; geese would be converted to goose,e tc. (There is a less sophisticated version of lemmatizer called a stemmer which just chops words to convert to the same base work; it doesn't work as well as a lemmatizer and we dont use it here.) There is a good description of the NLTK tokenizer [here](https://berkeley-stat159-f17.github.io/stat159-f17/lectures/11-strings/11-nltk..html).\n",
    "\n",
    "The output of this part is a cleaned up list of tokens for each tweet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/venkyubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/venkyubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "#\n",
    "# you may need to run the following\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set has two columns - screen_name and text (which is the raw tweet)\n",
    "\n",
    "## load tweets\n",
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
    "\n",
    "## screen_namee (accounts)\n",
    "#  democrat - hillary, time kaine, TheDemocrats\n",
    "# republicans - trunp, pence, GOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GOP', 'TheDemocrats', 'HillaryClinton', 'timkaine', 'mike_pence',\n",
       "       'realDonaldTrump'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['screen_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOP</td>\n",
       "      <td>RT @GOPconvention: #Oregon votes today. That m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheDemocrats</td>\n",
       "      <td>RT @DWStweets: The choice for 2016 is clear: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>Trump's calling for trillion dollar tax cuts f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>.@TimKaine's guiding principle: the belief tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>timkaine</td>\n",
       "      <td>Glad the Senate could pass a #THUD / MilCon / ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      screen_name                                               text\n",
       "0             GOP  RT @GOPconvention: #Oregon votes today. That m...\n",
       "1    TheDemocrats  RT @DWStweets: The choice for 2016 is clear: W...\n",
       "2  HillaryClinton  Trump's calling for trillion dollar tax cuts f...\n",
       "3  HillaryClinton  .@TimKaine's guiding principle: the belief tha...\n",
       "4        timkaine  Glad the Senate could pass a #THUD / MilCon / ..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13000</td>\n",
       "      <td>13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>12982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2217</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            screen_name                       text\n",
       "count             13000                      13000\n",
       "unique                6                      12982\n",
       "top     realDonaldTrump  MAKE AMERICA GREAT AGAIN!\n",
       "freq               2217                          4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13000</td>\n",
       "      <td>13000</td>\n",
       "      <td>13000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>12982</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2217</td>\n",
       "      <td>4</td>\n",
       "      <td>6554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            screen_name                       text  label\n",
       "count             13000                      13000  13000\n",
       "unique                6                      12982      2\n",
       "top     realDonaldTrump  MAKE AMERICA GREAT AGAIN!  False\n",
       "freq               2217                          4   6554"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add labels\n",
    "# 1 for D's\n",
    "# 0 for R's\n",
    "tweets['label'] = tweets['screen_name'].str.contains('TheDemocrats|HillaryClinton|timkaine', regex=True)\n",
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has 13K tweets, and each of the two classes have about an equal number of tweets.\n",
    "\n",
    "Now we will define our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#\n",
    "#  Input : dataframe with a column names 'text' which contains raw tweets (one per row)\n",
    "#  Output: A list of lists of tokens corrsponding to the 'text' column\n",
    "#\n",
    "def tokenize_tweets2(tweets):\n",
    "    \"\"\"Given a df with tweets in 'text' col, this function return tokens as a list of lists\"\"\"\n",
    "\n",
    "    # apply tokenize to the 'text' coolumn in the tweets df\n",
    "    tweet_tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    tokens = tweets['text'].apply(tweet_tokenizer.tokenize)\n",
    "    \n",
    "    # filter\n",
    "    misc = ['rt', '’', '…', '—', 'u', '”', 'w', '“', '...', '️', 'http', 'https']\n",
    "    to_remove = nltk.corpus.stopwords.words('english') + list(string.punctuation) + misc\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = [[lemmatizer.lemmatize(token) for token in tw if token not in to_remove] for tw in tokens]      \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['#oregon', 'vote', 'today', 'mean', '62', 'day', 'https://t.co/OoH9FVb7QS'],\n",
       " ['choice',\n",
       "  '2016',\n",
       "  'clear',\n",
       "  'need',\n",
       "  'another',\n",
       "  'democrat',\n",
       "  'white',\n",
       "  'house',\n",
       "  '#demdebate',\n",
       "  '#wearedemocrats',\n",
       "  'http://t.co/0n5g0YN46f'],\n",
       " [\"trump's\",\n",
       "  'calling',\n",
       "  'trillion',\n",
       "  'dollar',\n",
       "  'tax',\n",
       "  'cut',\n",
       "  'wall',\n",
       "  'street',\n",
       "  'time',\n",
       "  'pay',\n",
       "  'fair',\n",
       "  'share',\n",
       "  'https://t.co/y8vyESIOES'],\n",
       " ['guiding',\n",
       "  'principle',\n",
       "  'belief',\n",
       "  'make',\n",
       "  'difference',\n",
       "  'public',\n",
       "  'service',\n",
       "  'https://t.co/YopSUeMqOX'],\n",
       " ['glad',\n",
       "  'senate',\n",
       "  'could',\n",
       "  'pas',\n",
       "  '#thud',\n",
       "  'milcon',\n",
       "  'vetaffairs',\n",
       "  'approps',\n",
       "  'bill',\n",
       "  'solid',\n",
       "  'provision',\n",
       "  'virginia',\n",
       "  'https://t.co/NxIgRC3hDi'],\n",
       " ['exclusive',\n",
       "  'sits',\n",
       "  'see',\n",
       "  'sunday',\n",
       "  'morning',\n",
       "  '8:',\n",
       "  '30a',\n",
       "  'rtv',\n",
       "  '6',\n",
       "  'rtv',\n",
       "  '6',\n",
       "  'app'],\n",
       " ['chatham',\n",
       "  'town',\n",
       "  'council',\n",
       "  'congress',\n",
       "  'made',\n",
       "  'strong',\n",
       "  'mark',\n",
       "  'community',\n",
       "  'proud',\n",
       "  'work',\n",
       "  'together',\n",
       "  'behalf',\n",
       "  'va'],\n",
       " ['thank',\n",
       "  'new',\n",
       "  'orleans',\n",
       "  'louisiana',\n",
       "  '#makeamericagreatagain',\n",
       "  '#votetrump',\n",
       "  'https://t.co/tI1h9xT9GX',\n",
       "  'https://t.co/0bf7BOlWEj'],\n",
       " ['happy', '241st', 'birthday', 'thank', 'https://t.co/mXsxkfcstC'],\n",
       " ['excited',\n",
       "  'announce',\n",
       "  'today',\n",
       "  'plan',\n",
       "  'build',\n",
       "  'indiana',\n",
       "  'neuro-diagnostic',\n",
       "  'institute',\n",
       "  'partnership',\n",
       "  'https://t.co/hy4yRC…']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = tokenize_tweets2(tweets)\n",
    "print(len(all_tokens))\n",
    "all_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer can still be improved, but we will go with this. \n",
    "\n",
    "Let's find the most common tokens, and we will use all tokens that at least occur 25 times as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hillary', 1159),\n",
       " ('trump', 1144),\n",
       " ('great', 749),\n",
       " ('clinton', 720),\n",
       " ('today', 709),\n",
       " ('make', 581),\n",
       " ('donald', 576),\n",
       " ('president', 564),\n",
       " ('day', 552),\n",
       " ('thank', 539),\n",
       " ('american', 512),\n",
       " ('new', 503),\n",
       " ('job', 503),\n",
       " ('u', 485),\n",
       " ('america', 480),\n",
       " ('people', 469),\n",
       " ('vote', 451),\n",
       " ('state', 442),\n",
       " ('get', 420),\n",
       " ('year', 415)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter([token for tokens in all_tokens for token in tokens])\n",
    "print(len(counts))\n",
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = [k for k in counts.keys() if counts.get(k) > 25]\n",
    "len(top_words)\n",
    "#print(top_words[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_words are our features.\n",
    "Now let's construct a feature matrix from these top words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Martix Construction\n",
    "\n",
    "Problem 1 (15 points) Compute feature matrix\n",
    "\n",
    "Now we will extract the features from the training data and construct a feature matrix. The bad news is this matrix can be very large. In our case it is about 13K X 1K, or about 13M x 4 bytes ~ 52M, which will easily fit in the RAM of your laptops, but the training set could have easily been 10x or 100x the current size, and the number of features 10x in which case you would be out of luck. The good news is this matrix is likely to be very sparse. In fact, each tweet is not likely to contain more than 10-20 tokens, so even if this matrix becomes very large, we would be okay if we use a sparse representation.\n",
    "\n",
    "In a sparse representation, only the non-zero entities and their indices are saved. Scipy provides [several formats](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html) for sparse matrices. In this assignment, it doesn't matter which one you use (in fact, we could have even used a dense matrix). However, since we have to sum along columns (or features), the most suitable one is [csc (or compressed sparse column) format](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.sparse.csc_matrix.html).\n",
    "\n",
    "To make it easier to estimate priors and likelihoods, we will construct two feature matrices - one for each for the two classes. For this, first we need to figure out how many data points are in each class.\n",
    "\n",
    "While setting elements of a csc_matrix you may get a 'SparseEfficiencyWarning'; you can ignore that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "num_feat = len(top_words)\n",
    "\n",
    "# set this to the correct values\n",
    "#republicans\n",
    "nTrainR = tweets.groupby(\"label\").get_group(False).label.count() \n",
    "#democrats\n",
    "nTrainD = tweets.groupby(\"label\").get_group(True).label.count()  \n",
    "\n",
    "\n",
    "#print(nTrainR)\n",
    "#print(nTrainD)\n",
    "\n",
    "# create sparse feature matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "rfmat = csc_matrix((nTrainR, num_feat), dtype=int)\n",
    "dfmat = csc_matrix((nTrainD, num_feat), dtype=int)\n",
    "\n",
    "\n",
    "#\n",
    "# populate rfmat and dfmat with the counts of the features\n",
    "# Remember: all tokens are not features\n",
    "#\n",
    "tokensR = tokenize_tweets2(tweets[tweets.label==False])\n",
    "tokensD = tokenize_tweets2(tweets[tweets.label==True])\n",
    "\n",
    "# a function that might be useful is <list>.index() \n",
    "#republicans\n",
    "cnt1=0\n",
    "for word in top_words:\n",
    "    cnt2=0\n",
    "    for token in tokensR:\n",
    "        if word in token:\n",
    "            rfmat.data=np.append(rfmat.data,token.count(word))\n",
    "            rfmat.indices=np.append(rfmat.indices,cnt2)\n",
    "        cnt2 = cnt2 + 1\n",
    "    rfmat.indptr[cnt1+1]=len(rfmat.data)\n",
    "    cnt1=cnt1+1\n",
    "\n",
    "#democrats\n",
    "cnt1=0\n",
    "for word in top_words:\n",
    "    cnt2=0\n",
    "    for token in tokensD:\n",
    "        if word in token:\n",
    "            dfmat.data=np.append(dfmat.data,token.count(word))\n",
    "            dfmat.indices=np.append(dfmat.indices,cnt2)\n",
    "        cnt2 = cnt2 + 1\n",
    "    dfmat.indptr[cnt1+1]=len(dfmat.data)\n",
    "    cnt1=cnt1+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (11, 0)\t1\n",
      "  (65, 0)\t1\n",
      "  (74, 0)\t1\n",
      "  (96, 0)\t1\n",
      "  (133, 0)\t1\n",
      "  (166, 0)\t1\n",
      "  (182, 0)\t1\n",
      "  (225, 0)\t1\n",
      "  (236, 0)\t1\n",
      "  (243, 0)\t1\n",
      "  (250, 0)\t1\n",
      "  (302, 0)\t1\n",
      "  (315, 0)\t1\n",
      "  (319, 0)\t1\n",
      "  (337, 0)\t1\n",
      "  (400, 0)\t1\n",
      "  (411, 0)\t1\n",
      "  (418, 0)\t1\n",
      "  (515, 0)\t1\n",
      "  (532, 0)\t1\n",
      "  (535, 0)\t1\n",
      "  (552, 0)\t1\n",
      "  (599, 0)\t1\n",
      "  (602, 0)\t1\n",
      "  (610, 0)\t1\n",
      "  :\t:\n",
      "  (4646, 924)\t2\n",
      "  (1274, 925)\t1\n",
      "  (1382, 925)\t1\n",
      "  (2471, 925)\t1\n",
      "  (2659, 925)\t1\n",
      "  (3232, 925)\t1\n",
      "  (3729, 925)\t1\n",
      "  (4019, 925)\t1\n",
      "  (4873, 925)\t1\n",
      "  (4899, 925)\t1\n",
      "  (5573, 925)\t1\n",
      "  (5857, 925)\t1\n",
      "  (6072, 925)\t1\n",
      "  (1408, 926)\t1\n",
      "  (2290, 926)\t1\n",
      "  (2306, 926)\t1\n",
      "  (2313, 926)\t1\n",
      "  (2735, 926)\t1\n",
      "  (3899, 926)\t1\n",
      "  (4776, 926)\t1\n",
      "  (5192, 926)\t1\n",
      "  (5482, 926)\t1\n",
      "  (5523, 926)\t1\n",
      "  (5682, 926)\t1\n",
      "  (6215, 926)\t1\n"
     ]
    }
   ],
   "source": [
    "print(dfmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (31, 0)\t1\n",
      "  (98, 0)\t1\n",
      "  (212, 0)\t1\n",
      "  (236, 0)\t1\n",
      "  (247, 0)\t1\n",
      "  (326, 0)\t1\n",
      "  (401, 0)\t1\n",
      "  (552, 0)\t1\n",
      "  (570, 0)\t1\n",
      "  (605, 0)\t1\n",
      "  (617, 0)\t1\n",
      "  (628, 0)\t1\n",
      "  (646, 0)\t1\n",
      "  (678, 0)\t1\n",
      "  (782, 0)\t1\n",
      "  (789, 0)\t1\n",
      "  (806, 0)\t1\n",
      "  (825, 0)\t2\n",
      "  (870, 0)\t1\n",
      "  (988, 0)\t1\n",
      "  (1010, 0)\t1\n",
      "  (1037, 0)\t1\n",
      "  (1039, 0)\t1\n",
      "  (1152, 0)\t1\n",
      "  :\t:\n",
      "  (4454, 925)\t1\n",
      "  (4532, 925)\t1\n",
      "  (5295, 925)\t1\n",
      "  (5534, 925)\t1\n",
      "  (5721, 925)\t1\n",
      "  (5951, 925)\t1\n",
      "  (6397, 925)\t1\n",
      "  (6447, 925)\t1\n",
      "  (1385, 926)\t1\n",
      "  (1428, 926)\t1\n",
      "  (1578, 926)\t1\n",
      "  (1848, 926)\t1\n",
      "  (1908, 926)\t1\n",
      "  (2872, 926)\t1\n",
      "  (2896, 926)\t1\n",
      "  (2944, 926)\t1\n",
      "  (3470, 926)\t1\n",
      "  (3898, 926)\t2\n",
      "  (4047, 926)\t1\n",
      "  (4304, 926)\t1\n",
      "  (4438, 926)\t1\n",
      "  (5105, 926)\t1\n",
      "  (5160, 926)\t1\n",
      "  (5229, 926)\t1\n",
      "  (5686, 926)\t1\n"
     ]
    }
   ],
   "source": [
    "print(rfmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Naive Bayes Model Parameters\n",
    "\n",
    "Problem 2 (5 points) compute log priors\n",
    "\n",
    "Problem 3 (30 points) compute log likelihoods using Laplace smoothing\n",
    "\n",
    "Now we can compute the model parameters, this is, the likelihoods and priors for the two classes. As we discussed in class, since the probabilities can be very small numbers, we will compute log likelihoods and log priors. Aslo use Laplace (aka add one) smoothing.\n",
    "\n",
    "To sum a matrix column, you can use something like dfmat[:,i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000\n",
      "Log of priors of Republicans:  -0.6848738071849139\n",
      "Log of priors of Democrats:  -0.7014895740682907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venkyubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "/home/venkyubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "# compute log priors\n",
    "import math as m\n",
    "total_count = tweets[\"label\"].count()\n",
    "print(total_count)\n",
    "\n",
    "lPriorR=m.log((nTrainR)/total_count)\n",
    "lPriorD=m.log((nTrainD)/total_count)\n",
    "\n",
    "print(\"Log of priors of Republicans: \",lPriorR)\n",
    "print(\"Log of priors of Democrats: \",lPriorD)\n",
    "\n",
    "\n",
    "# compute log likelihoods\n",
    "#republicans\n",
    "featureR = np.array([])\n",
    "\n",
    "for i in range(0,len(top_words)):\n",
    "    featureR = np.append(featureR, rfmat[:,i].sum())\n",
    "fLikelihoodR = (featureR + 1)/(rfmat.sum() + 2) #laplace smoothing, here k=2, 2 values for a feature\n",
    "fLikelihoodR_Nolaplace = (featureR)/(rfmat.sum())\n",
    "\n",
    "#print(fLikelihoodR)\n",
    "\n",
    "logLikelihoodR = np.log(fLikelihoodR) \n",
    "#print(logLikelihoodR)\n",
    "\n",
    "logLikelihoodR_Nolaplace = np.log(fLikelihoodR_Nolaplace) \n",
    "\n",
    "#democrats\n",
    "featureD = np.array([])\n",
    "\n",
    "for i in range(0,len(top_words)):\n",
    "    featureD = np.append(featureD, dfmat[:,i].sum())\n",
    "fLikelihoodD = (featureD + 1)/(dfmat.sum() + 2) #laplace smoothing\n",
    "fLikelihoodD_Nolaplace = (featureD)/(dfmat.sum())\n",
    "#print(fLikelihoodD)\n",
    "\n",
    "logLikelihoodD = np.log(fLikelihoodD) \n",
    "#print(logLikelihoodD)\n",
    "\n",
    "logLikelihoodD_Nolaplace = np.log(fLikelihoodD_Nolaplace) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Set\n",
    "\n",
    "Now we have a trained Naive Bayes classifier. We will load the test data set and make the predictions. Note: If a token is not a feature, ignore it. \n",
    "\n",
    "Problem 4 (5 points) Load test data and tokenize\n",
    "\n",
    "Problem 5 (30 points) Using the trained NB classifier predict the labels\n",
    "\n",
    "Problem 6 (5 points) Calculate accuracy, recall, and precision of your predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['staff', 'hosting', 'office', 'hour', 'across', 'virginia', 'next', 'week', 'answer', 'question', 'find', 'location', 'near', 'https://t.co/nulOEkTOKB'], ['enjoyed', 'r', 'community', 'convo', 'today', 'special', 'thx', 'covenant', 'christian', 'h', 'demotte', 'coming', 'https://…']]\n"
     ]
    }
   ],
   "source": [
    "#Prob 4\n",
    "tweetsT = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
    "\n",
    "tweetsT['label'] = tweetsT['screen_name'].str.contains('TheDemocrats|HillaryClinton|timkaine', regex=True)\n",
    "tweetsT.describe()\n",
    "\n",
    "testToken = tokenize_tweets2(tweetsT)\n",
    "print(testToken[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#Prob 5\n",
    "prediction = np.array([]) #final array where prediction is stored\n",
    "\n",
    "tempR = 0\n",
    "tempD = 0\n",
    "log_pred_prob_R = np.array([])\n",
    "log_pred_prob_D = np.array([])\n",
    "\n",
    "for each_tweet in testToken:\n",
    "    for each_word in each_tweet:\n",
    "        if each_word in top_words:\n",
    "            tempR = tempR + logLikelihoodR[top_words.index(each_word)]\n",
    "            tempD = tempD + logLikelihoodD[top_words.index(each_word)]\n",
    "    tempR = tempR + lPriorR\n",
    "    tempD = tempD + lPriorR\n",
    "    \n",
    "    log_pred_prob_R = np.append(log_pred_prob_R, tempR)\n",
    "    log_pred_prob_D = np.append(log_pred_prob_D, tempD)\n",
    "    \n",
    "    if(tempR < tempD):\n",
    "        prediction = np.append(prediction, True)\n",
    "    elif((tempR > tempD)):\n",
    "        prediction = np.append(prediction, False)\n",
    "    else:\n",
    "        # if log prob for predictions is equal, predicting answer on basis of prior probability\n",
    "        prediction = np.append(prediction, False)\n",
    "    \n",
    "    tempR = 0\n",
    "    tempD = 0\n",
    "    \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8108422522103303\n",
      "Recall =  0.8109700815956482\n",
      "Precision =  0.8187643020594966\n"
     ]
    }
   ],
   "source": [
    "# problem 6\n",
    "#initializing variables for true-positives, true-negatives, false-nagatives and false-positives\n",
    "TP=0\n",
    "TN=0\n",
    "FN=0\n",
    "FP=0\n",
    "label = tweetsT[\"label\"]   \n",
    "for i in range(len(tweetsT)):\n",
    "    if prediction[i]==True and label[i]==True:\n",
    "        TP=TP+1\n",
    "    elif prediction[i]==False and label[i]==False:\n",
    "        TN=TN+1\n",
    "    elif prediction[i]==False and label[i]==True:\n",
    "        FN=FN+1\n",
    "    elif prediction[i]==True and label[i]==False:\n",
    "        FP=FP+1\n",
    "print('Accuracy = ',(TP+TN)/(TP+TN+FN+FP))\n",
    "print('Recall = ',(TP)/(TP+FN))\n",
    "print('Precision = ',(TP)/(TP+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7 (5 points) List the features with top ten likelihoods for each of the two classes. What is the likelihood for 'hillary', that is, P(hillary|class)? Is it in the top ten? How important is it in this classification problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "The likehood for 'hillary' is in the top ten likelihoods for each of the two classes.\n",
    "It has same values in both the classes. Hence it is not an important deciding factor.\n",
    "P(feature = 'hillary' | Republican) = -4.1629405295561934\n",
    "P(feature = 'hillary' | Democrat) = -4.2252911744789374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republicans: top ten likelihoods\n",
      " \n",
      "('clinton', -4.1410725429196127)\n",
      "('hillary', -4.1629405295561934)\n",
      "('great', -4.1940311166262241)\n",
      "('thank', -4.439348525434756)\n",
      "('today', -4.6591344910541199)\n",
      "('day', -4.7166215819718014)\n",
      "('new', -4.7166215819718014)\n",
      "('indiana', -4.7375491020777574)\n",
      "('job', -4.7807657035775399)\n",
      "('state', -4.7807657035775399)\n",
      "Democrats: top ten likelihoods\n",
      " \n",
      "('trump', -3.8088337659282852)\n",
      "('hillary', -4.2252911744789374)\n",
      "('donald', -4.3242730080277889)\n",
      "('president', -4.5698237315919448)\n",
      "('today', -4.7097088342603799)\n",
      "('american', -4.7211049689912503)\n",
      "('make', -4.7472312735834699)\n",
      "('u', -4.8525917892412966)\n",
      "('vote', -4.8790834046882727)\n",
      "('one', -5.050723952061893)\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "likelihoodR = {}\n",
    "likelihoodD = {}\n",
    "\n",
    "for i in range(0 , len(top_words)):\n",
    "    likelihoodR[top_words[i]] = logLikelihoodR[i]\n",
    "    likelihoodD[top_words[i]] = logLikelihoodD[i]\n",
    "    \n",
    "#print(feat_likelihood_R)\n",
    "slikelihoodR = sorted(likelihoodR.items(), key=operator.itemgetter(1), reverse = True)\n",
    "print(\"Republicans: top ten likelihoods\\n \")\n",
    "for i in slikelihoodR[:10]:\n",
    "    print(i)\n",
    "\n",
    "slikelihoodD = sorted(likelihoodD.items(), key=operator.itemgetter(1), reverse = True)\n",
    "print(\"Democrats: top ten likelihoods\\n \")\n",
    "for i in slikelihoodD[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8 (5 points) How important are the priors in this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: Looking at the prior values, it seems they do not play a significant part in calculating the posteriors. They have almost equal and uniform values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra credit (5 points): Compute the accuracy of the test set without Laplace smoothing and compare with the above.\n",
    "It seems from the observation that the accuracy does not change very much after remove Laplace function. Only difference is that if Laplace is not used, for features that have not appeared even once in a particular class will throw error/warnings as their sum will be 0 and log(0) is undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0. ...,  0.  0.  0.]\n",
      "1784   1693   422   399\n",
      "Accuracy =  0.8089809213587715\n",
      "Recall =  0.8087035358114234\n",
      "Precision =  0.8172240036646816\n"
     ]
    }
   ],
   "source": [
    "# extra credit problem \n",
    "prediction = np.array([]) #final array where prediction is stored\n",
    "\n",
    "tempR = 0\n",
    "tempD = 0\n",
    "log_pred_prob_R = np.array([])\n",
    "log_pred_prob_D = np.array([])\n",
    "\n",
    "for each_tweet in testToken:\n",
    "    for each_word in each_tweet:\n",
    "        if each_word in top_words:\n",
    "            tempR = tempR + logLikelihoodR_Nolaplace[top_words.index(each_word)]\n",
    "            tempD = tempD + logLikelihoodD_Nolaplace[top_words.index(each_word)]\n",
    "    tempR = tempR + lPriorR\n",
    "    tempD = tempD + lPriorR\n",
    "    \n",
    "    log_pred_prob_R = np.append(log_pred_prob_R, tempR)\n",
    "    log_pred_prob_D = np.append(log_pred_prob_D, tempD)\n",
    "    \n",
    "    if(tempR < tempD):\n",
    "        prediction = np.append(prediction, True)\n",
    "    elif((tempR > tempD)):\n",
    "        prediction = np.append(prediction, False)\n",
    "    else:\n",
    "        # if log prob for predictions is equal, predicting answer on basis of prior probability\n",
    "        prediction = np.append(prediction, False)\n",
    "    \n",
    "    tempR = 0\n",
    "    tempD = 0\n",
    "    \n",
    "print(prediction)\n",
    "\n",
    "#initializing variables for true-positives, true-negatives, false-nagatives and false-positives\n",
    "TP=0\n",
    "TN=0\n",
    "FN=0\n",
    "FP=0\n",
    "label = tweetsT[\"label\"]    # extracting label column to a list; predictions already in list \"result\"\n",
    "for i in range(len(tweetsT)):\n",
    "    if prediction[i]==True and label[i]==True:\n",
    "        TP=TP+1\n",
    "    elif prediction[i]==False and label[i]==False:\n",
    "        TN=TN+1\n",
    "    elif prediction[i]==False and label[i]==True:\n",
    "        FN=FN+1\n",
    "    elif prediction[i]==True and label[i]==False:\n",
    "        FP=FP+1\n",
    "print(TP,\" \", TN, \" \",FN,\" \",FP)\n",
    "print('Accuracy = ',(TP+TN)/(TP+TN+FN+FP))\n",
    "print('Recall = ',(TP)/(TP+FN))\n",
    "print('Precision = ',(TP)/(TP+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
